.kafka-sink-data-formats

=================================
Kafka Sink Connector Data Formats
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol


Supported Data Formats
----------------------

The MongoDB Kafka Sink Connector supports the following data serialization
formats for reading from Kafka topics:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1 4

   * - Format Name
     - Description

   * - Avro
     -  Offers a compact binary format to provide smaller message sizes and
       a JSON-like API. Uses the `Confluent Schema Registry
       <https://www.confluent.io/confluent-schema-registry>`_ to manage
       schema definitions. 

   * - JSON with Schema
     - Offers JSON record structure with explicit schema information

   * - JSON (plain)
     - Offers JSON record structure without any attached schema
       information

   * - RAW JSON
     - Serialized as a String only. The JSON structure will not be managed
       by Kafka Connect.

       .. note::

          This format requires that the field contain valid JSON even
          when specifying the `StringConverter
          <https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_

These settings are defined 
Since key and value settings can be independently configured, it is possible
to work with different data formats for keys and values in a Kafka topic.

.. note::

   **RAW JSON** mode 
   requires the Strings to be valid JSON
   Even when using RAW JSON mode i.e. with `StringConverter
   <https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_
   the expected Strings have to be valid JSON.


See `Kafka Connect Serialization Explained
<https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained#choosing-serialization-format>`_
for more information on Kafka data serialization.


For more information on converters

Configuration example for AVRO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: properties

   key.converter=io.confluent.connect.avro.AvroConverter
   key.converter.schema.registry.url=http://localhost:8081

   value.converter=io.confluent.connect.avro.AvroConverter
   value.converter.schema.registry.url=http://localhost:8081


Configuration example for JSON with Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: properties

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=true

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=true


Supported Sink Record Structure
-------------------------------

Currently the connector is able to process Kafka Connect ``SinkRecord``
objects with support for the following schema types `Schema.Type
<https://kafka.apache.org/21/javadoc/org/apache/kafka/connect/data/Schema.Type.html>`_:
*INT8, INT16, INT32, INT64, FLOAT32, FLOAT64, BOOLEAN, STRING, BYTES,
ARRAY, MAP, STRUCT.*

The conversion is able to generically deal with nested key or value structures
based on the supported types listed above. The following example is based on
`AVRO <https://avro.apache.org/>`_:

.. code-block:: json

   {
     "type": "record",
     "name": "Customer",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
     "fields": [
       {
         "name": "name",
         "type": "string"
       },
       {
         "name": "age",
         "type": "int"
       },
       {
         "name": "active",
         "type": "boolean"
       },
       {
         "name": "address",
         "type": {
           "type": "record",
           "name": "AddressRecord",
           "fields": [
             {
               "name": "city",
               "type": "string"
             },
             {
               "name": "country",
               "type": "string"
             }
           ]
         }
       },
       {
         "name": "food",
         "type": {
           "type": "array",
           "items": "string"
         }
       },
       {
         "name": "data",
         "type": {
           "type": "array",
           "items": {
             "type": "record",
             "name": "Whatever",
             "fields": [
               {
                 "name": "k",
                 "type": "string"
               },
               {
                 "name": "v",
                 "type": "int"
               }
             ]
           }
         }
       },
       {
         "name": "lut",
         "type": {
           "type": "map",
           "values": "double"
         }
       },
       {
         "name": "raw",
         "type": "bytes"
       }
     ]
   }

Logical Types
~~~~~~~~~~~~~

In addition to the Kafka standard schema types, you can apply the
following `AVRO logical types
<http://avro.apache.org/docs/1.8.2/spec.html#Logical+Types>`_:

- **Decimal**
- **Date**
- **Time** (millis/micros)
- **Timestamp** (millis/micros)

See the following AVRO schema snippet for examples of each type:

.. code-block:: json

   {
     "type": "record",
     "name": "MyLogicalTypesRecord",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
       "fields": [
         {
           "name": "myDecimalField",
           "type": {
             "type": "bytes",
             "logicalType": "decimal",
             "connect.parameters": {
               "scale": "2"
             }
           }
         },
         {
           "name": "myDateField",
           "type": {
             "type": "int",
             "logicalType": "date"
           }
         },
         {
           "name": "myTimeMillisField",
           "type": {
             "type": "int",
             "logicalType": "time-millis"
           }
         },
         {
           "name": "myTimeMicrosField",
           "type": {
             "type": "long",
             "logicalType": "time-micros"
           }
         },
         {
           "name": "myTimestampMillisField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-millis"
           }
         },
         {
           "name": "myTimestampMicrosField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-micros"
           }
         }
       ]
     }

.. note::

   If you are using AVRO code generation for logical types in order to
   use them from a Java-based producer app you end-up with the following Java
   type mappings:

   - ``org.joda.time.LocalDate myDateField``
   - ``org.joda.time.LocalTime mytimeMillisField``
   - ``long myTimeMicrosField``
   - ``org.joda.time.DateTime myTimestampMillisField``
   - ``long myTimestampMicrosField``

.. note::

   AVRO 1.9.0 will support native Java 8 date time types. Logical types
   can only be supported for **AVRO** and **JSON + Schema** data.



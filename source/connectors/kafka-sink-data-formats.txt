.kafka-sink-data-formats

=================================
Kafka Sink Connector Data Formats
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol


Supported Data Formats
----------------------

The MongoDB Kafka Sink Connector ``converter`` setting specifies the
deserialization method for data it reads from a topic. The converter
can deserialize the following data formats:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 1 4

   * - Format Name
     - Description

   * - Avro
     - An `open source serialization system <https://avro.apache.org>`_
       that provides a compact binary format and a JSON-like API.
       Integrates with the
       `Confluent Schema Registry <https://www.confluent.io/confluent-schema-registry>`_
       to manage schema definitions.

   * - JSON with Schema
     - JSON record structure with explicit schema information to
       ensure the data matches the expected format.

   * - JSON (plain)
     - JSON record structure without an attached schema.

   * - RAW JSON
     - Serialized as a String. The JSON structure is not managed by
       Kafka Connect.

       .. note::

          This format requires that the fields contain valid JSON even
          when specifying the `StringConverter
          <https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_

.. note::

   **RAW JSON** mode
   requires the Strings to be valid JSON
   Even when using RAW JSON mode, i.e. with `StringConverter
   <https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_,
   the expected Strings have to be valid JSON.

For more information on Kafka data serialization, see `Kafka Connect
Serialization Explained
<https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained#choosing-serialization-format>`_.

Configuration Example for AVRO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the
**AVRO data format** with a Schema Registry:

.. code-block:: properties

   key.converter=io.confluent.connect.avro.AvroConverter
   key.converter.schema.registry.url=http://localhost:8081

   value.converter=io.confluent.connect.avro.AvroConverter
   value.converter.schema.registry.url=http://localhost:8081

For more information on using a Schema Registry, see `Schema Management
<https://docs.confluent.io/current/schema-registry/index.html>`_.

Configuration Example for JSON with Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the
**JSON with schema** data format. The Kafka topic data must be in
JSON format and contain top-level objects ``schema`` and ``payload``.

.. code-block:: properties

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=true

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=true

Configuration Example for JSON without Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the **JSON
without schema** data format. The Kafka topic data must be in JSON format.

.. code-block:: properties

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=false

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=false

.. admonition:: Choose the appropriate format
   :class: note

   If a top-level JSON object named ``schema`` or ``payload`` is
   included in in the message, it will be parsed literally instead of as
   a validation schema.

Configuration Example for RAW JSON
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following configuration provides example settings that use the
**RAW JSON** data format.

.. code-block:: properties

   key.converter=org.apache.kafka.connect.storage.StringConverter
   key.converter.schemas.enable=false

   value.converter=org.apache.kafka.connect.storage.StringConverter
   value.converter.schemas.enable=false

Supported Sink Record Structure
-------------------------------

After getting converted, a SinkRecord is created.
The SinkRecord

Once the converter has deserialized the data, Kafka Connect creates
a :kafka-21-javadoc:`SinkRecord </connect/sink/SinkRecord.html>` object.
The MongoDB Kafka Connector
supports the following data types in the schema.


The connector is able to process Kafka Connect ``SinkRecord``
objects with support for all the schema types defined in
:kafka-21-javadoc:`Schema.Type </connect/data/Schema.Type.html>`
(``INT8``, ``INT16``, ``INT32``, ``INT64``, ``FLOAT32``, ``FLOAT64``,
``BOOLEAN``, ``STRING``, ``BYTES``, ``ARRAY``, ``MAP``, and ``STRUCT``)

The converter handles nested key or value structures
The conversion is able to generically deal with nested key or value structures
based on the supported types listed above. The following example is based on
`AVRO <https://avro.apache.org/>`_:

.. code-block:: json

   {
     "type": "record",
     "name": "Customer",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
     "fields": [
       {
         "name": "name",
         "type": "string"
       },
       {
         "name": "age",
         "type": "int"
       },
       {
         "name": "active",
         "type": "boolean"
       },
       {
         "name": "address",
         "type": {
           "type": "record",
           "name": "AddressRecord",
           "fields": [
             {
               "name": "city",
               "type": "string"
             },
             {
               "name": "country",
               "type": "string"
             }
           ]
         }
       },
       {
         "name": "food",
         "type": {
           "type": "array",
           "items": "string"
         }
       },
       {
         "name": "data",
         "type": {
           "type": "array",
           "items": {
             "type": "record",
             "name": "Whatever",
             "fields": [
               {
                 "name": "k",
                 "type": "string"
               },
               {
                 "name": "v",
                 "type": "int"
               }
             ]
           }
         }
       },
       {
         "name": "lut",
         "type": {
           "type": "map",
           "values": "double"
         }
       },
       {
         "name": "raw",
         "type": "bytes"
       }
     ]
   }

AVRO Logical Types
~~~~~~~~~~~~~~~~~~

In addition to the Kafka standard schema types, you can specify the
following `AVRO logical types
<http://avro.apache.org/docs/1.8.2/spec.html#Logical+Types>`_:

- **Decimal**
- **Date**
- **Time** (millis/micros)
- **Timestamp** (millis/micros)

See the following AVRO schema snippet for examples of each type:

.. code-block:: json

   {
     "type": "record",
     "name": "MyLogicalTypesRecord",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
       "fields": [
         {
           "name": "myDecimalField",
           "type": {
             "type": "bytes",
             "logicalType": "decimal",
             "connect.parameters": {
               "scale": "2"
             }
           }
         },
         {
           "name": "myDateField",
           "type": {
             "type": "int",
             "logicalType": "date"
           }
         },
         {
           "name": "myTimeMillisField",
           "type": {
             "type": "int",
             "logicalType": "time-millis"
           }
         },
         {
           "name": "myTimeMicrosField",
           "type": {
             "type": "long",
             "logicalType": "time-micros"
           }
         },
         {
           "name": "myTimestampMillisField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-millis"
           }
         },
         {
           "name": "myTimestampMicrosField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-micros"
           }
         }
       ]
     }

.. note::

   If you are using AVRO code generation for logical types in order to
   use them from a Java-based producer app you end-up with the following Java
   type mappings:

   - ``org.joda.time.LocalDate myDateField``
   - ``org.joda.time.LocalTime mytimeMillisField``
   - ``long myTimeMicrosField``
   - ``org.joda.time.DateTime myTimestampMillisField``
   - ``long myTimestampMicrosField``

.. note::

   AVRO 1.9.0 will support native Java 8 date time types. Logical types
   can only be supported for **AVRO** and **JSON + Schema** data.



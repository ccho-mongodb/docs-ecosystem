.kafka-sink-data-formats

=================================
Kafka Sink Connector Data Formats
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Sink Data Formats
-----------------


Supported Data Formats
----------------------

The sink connector implementation is configurable in order to support:

- **AVRO** (makes use of Confluent's Kafka Schema Registry and is the
  recommended format)
- **JSON with Schema** (offers JSON record structure with explicit schema
  information)
- **JSON plain** (offers JSON record structure without any attached schema)
- **RAW JSON** (string only - JSON structure not managed by Kafka connect)

Since key and value settings can be independently configured, it is possible
to work with different data formats for records' keys and values respectively.

.. note:: 
   Even when using RAW JSON mode i.e. with `StringConverter
   <https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_
   the expected Strings have to be valid JSON.

See the excellent Confluent post `Serializers Explained
<https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained>`_
for more information about Kafka data serialization.

Supported Sink Record Structure
-------------------------------

Currently the connector is able to process Kafka Connect ``SinkRecord``
objects with support for the following schema types `Schema.Type
<https://kafka.apache.org/21/javadoc/org/apache/kafka/connect/data/Schema.Type.html>`_:
*INT8, INT16, INT32, INT64, FLOAT32, FLOAT64, BOOLEAN, STRING, BYTES,
ARRAY, MAP, STRUCT.*

The conversion is able to generically deal with nested key or value structures
based on the supported types listed above. The following example is based on
`AVRO <https://avro.apache.org/>`_:

.. code-block:: json

   {
     "type": "record",
     "name": "Customer",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
     "fields": [
       {
         "name": "name",
         "type": "string"
       },
       {
         "name": "age",
         "type": "int"
       },
       {
         "name": "active",
         "type": "boolean"
       },
       {
         "name": "address",
         "type": {
           "type": "record",
           "name": "AddressRecord",
           "fields": [
             {
               "name": "city",
               "type": "string"
             },
             {
               "name": "country",
               "type": "string"
             }
           ]
         }
       },
       {
         "name": "food",
         "type": {
           "type": "array",
           "items": "string"
         }
       },
       {
         "name": "data",
         "type": {
           "type": "array",
           "items": {
             "type": "record",
             "name": "Whatever",
             "fields": [
               {
                 "name": "k",
                 "type": "string"
               },
               {
                 "name": "v",
                 "type": "int"
               }
             ]
           }
         }
       },
       {
         "name": "lut",
         "type": {
           "type": "map",
           "values": "double"
         }
       },
       {
         "name": "raw",
         "type": "bytes"
       }
     ]
   }

Logical Types
~~~~~~~~~~~~~

In addition to the Kafka standard schema types, you can apply the
following `AVRO logical types
<http://avro.apache.org/docs/1.8.2/spec.html#Logical+Types>`_:

- **Decimal**
- **Date**
- **Time** (millis/micros)
- **Timestamp** (millis/micros)

See the following AVRO schema snippet for examples of each type:

.. code-block:: json

   {
     "type": "record",
     "name": "MyLogicalTypesRecord",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
       "fields": [
         {
           "name": "myDecimalField",
           "type": {
             "type": "bytes",
             "logicalType": "decimal",
             "connect.parameters": {
               "scale": "2"
             }
           }
         },
         {
           "name": "myDateField",
           "type": {
             "type": "int",
             "logicalType": "date"
           }
         },
         {
           "name": "myTimeMillisField",
           "type": {
             "type": "int",
             "logicalType": "time-millis"
           }
         },
         {
           "name": "myTimeMicrosField",
           "type": {
             "type": "long",
             "logicalType": "time-micros"
           }
         },
         {
           "name": "myTimestampMillisField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-millis"
           }
         },
         {
           "name": "myTimestampMicrosField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-micros"
           }
         }
       ]
     }

.. note::

   If you are using AVRO code generation for logical types in order to
   use them from a Java-based producer app you end-up with the following Java
   type mappings:

   - ``org.joda.time.LocalDate myDateField``
   - ``org.joda.time.LocalTime mytimeMillisField``
   - ``long myTimeMicrosField``
   - ``org.joda.time.DateTime myTimestampMillisField``
   - ``long myTimestampMicrosField``

.. note::

   AVRO 1.9.0 will support native Java 8 date time types. Logical types
   can only be supported for **AVRO** and **JSON + Schema** data.

Configuration example for AVRO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: properties

   key.converter=io.confluent.connect.avro.AvroConverter
   key.converter.schema.registry.url=http://localhost:8081

   value.converter=io.confluent.connect.avro.AvroConverter
   value.converter.schema.registry.url=http://localhost:8081


Configuration example for JSON with Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: properties

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=true

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=true


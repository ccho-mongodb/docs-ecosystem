.. _kafka-sink:

==========================
Kafka Sink Connector Guide
==========================

.. default-domain:: mongodb

A Kafka Sink Connector consumes records from a Kafka topic and saves the
data to a datastore.

MongoDB Persistence
-------------------

The Kafka records are converted to Bson documents which are in turn inserted 
into the corresponding MongoDB target collection. According to the chosen write
model strategy either a `ReplaceOneModel
<http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/ReplaceOneModel.html>`_
or an `UpdateOneModel
<http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/UpdateOneModel.html>`_ 
will be used whenever inserts or updates are handled. Either model will perform
an upsert if the data does not exist in the collection.

If the connector is configured to process convention-based deletes, then when
``null`` values for Kakfa records are discovered a `DeleteOneModel
<http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/client/model/DeleteOneModel.html>`_
will be used.

Data is written using the configured write concern level of the connection as 
specified in the `connection string
<http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/ConnectionString.html>`_.
If the bulk write fails (totally or partially) errors are logged and a simple
retry logic is in place.

Sink Connector Configuration Properties
---------------------------------------

The following settings can be configured in the ``connector.properties`` file.
For an example configuration file see `MongoSinkConnector.properties
<https://github.com/mongodb/mongo-kafka/blob/master/config/MongoSinkConnector.properties>`_.

.. list-table::
   :header-rows: 1

   * - Name
     - Description
     - Type
     - Default
     - Valid Values
     - Importance
     
   * - topics
     - A list of kafka topics for the sink connector.
     - list
     -
     - A non-empty list	
     - high

   * - connection.uri
     - The connection URI as supported by the official drivers. E.g. mongodb://user@pass@locahost/
     - string
     - mongodb://localhost:27017
     - A valid connection string
     - high
  
   * - database
     - The database for the sink to write.
     - string
     -
     - non-empty string	
     - high
     
   * - collection
     - Optional, single sink collection name to write to. If following multiple
       topics then this will be the default collection they are mapped to.
     - string
     - ""
     - 
     - high

   * - document.id.strategy
     - The IdStrategy class name to use for generating a unique document id 
       (_id).
     - string
     - com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
     - An empty string OR A string matching 
       ``(\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*\.)*\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*``
     - high

   * - delete.on.null.values
     - Whether or not the connector tries to delete documents based on key when 
       value is null.
     - boolean
     - false
     - 
     - medium
     
   * - max.batch.size
     - The maximum number of sink records to possibly batch together for 
       processing.
     - int
     - 0
     - [0,...]
     - medium

   * - max.num.retries
     - How often a retry should be done on write errors.
     - int
     - 3
     - [0,...]
     - medium
       
   * - retries.defer.timeout
     - How long in ms a retry should get deferred.
     - into
     - 5000
     - [0,...]
     - medium
 
   * - change.data.capture.handler
     - The class name of the CDC handler to use for processing.
     - string	
     - ""
     - An empty string OR A string matching
       ``(\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*\.)*\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*``
     - low

   * - field.renamer.mapping
     - An inline JSON array with objects describing field name mappings. 
       Example: 
       [{"oldName":"key.fieldA","newName":"field1"},{"oldName":"value.xyz",
       "newName":"abc"}]	
     - string	
     - []	
     - A valid JSON array
     - low
     
   * - field.renamer.regexp
     - An inline JSON array with objects describing regexp settings. Example:
       [[{"regexp":"^key\\\\..*my.*$","pattern":"my","replace":""},{"regexp":
       "^value\\\\..*$","pattern":"\\\\.","replace":"_"}]
     - string	
     - []
     - A valid JSON array
     - low


   * - key.projection.list
     - A comma separated list of field names for key projection.
     - string
     - ""
     - 
     - low
     
   * - key.projection.type
     - The type of key projection to use.
     - string
     - none
     - [none, blacklist, whitelist]
     - low
       
   * - post.processor.chain
     - A comma separated list of post processor classes to process the data 
       before saving to MongoDB.
     - list	
     - [ com.mongodb.kafka.connect.sink.processor.DocumentIdAdder ]
     - A list matching: (\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*\.)*\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*
     - low
     
   * - rate.limiting.every.n
     - After how many processed batches the rate limit should trigger (NO rate
       limiting if n=0)
     - int
     - 0
     - [0,...]
     - low

   * - rate.limiting.timeout
     - How long in ms processing should wait before continuing processing.
     - int
     - 0
     - [0,...]
     - low

   * - value.projection.list
     - A comma separated list of field names for value projection.
     - string	
     - ""
     -
     - low

   * - value.projection.type
     - The type of value projection to use.
     - string	
     - none	
     - [none, blacklist, whitelist]
     - low
     
   * - writemodel.strategy
     - The class the handles how build the write models for the sink
       documents.
     - string	
     - com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
     - A string matching (\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*\.)*\p{javaJavaIdentifierStart}\p{javaJavaIdentifierPart}*
     - low
       
   * - topic.override.%s.%s
     - The overrides configuration allows for per topic customization of 
       configuration. The customized overrides are merged with the default 
       configuration, to create the specific configuration for a topic. 
       For example, ``topic.override.foo.collection=bar`` will store data
       from the foo topic into the bar collection. Note: All configuration 
       options apart from 'connection.uri' and 'topics' are overridable.
     - string	
     - ""
     - Topic override
     - low

Topic Specific Configuration Settings
-------------------------------------

The MongoDB Kafka Sink Connector, supports sinking data from multiple topics.
However, as data may vary between the topics, individual configurations can 
be overriden using the ``topic.override.<topicName>.<configurationName>``
syntax. This allows any individual configuration to be overridden on a per
topic basis.

**Note:** The ``topics`` and ``connection.uri`` configurations are global and 
*cannot* be overridden.

The following configuration fragments show how to apply different settings for
the *topicA* and *topicC* topics.

.. code-block::

   # Specific processing settings for 'topicA'

   topic.override.topicA.collection=collectionA
   topic.override.topicA.document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.UuidStrategy
   topic.override.topicA.post.processor.chain=com.mongodb.kafka.connect.sink.processor.DocumentIdAdder,com.mongodb.kafka.connect.sink.processor.BlacklistValueProjector
   topic.override.topicA.value.projection.type=blacklist
   topic.override.topicA.value.projection.list=k2,k4
   topic.override.topicA.max.batch.size=100

These properties result in the following actions for messages originating 
from the 'topicA' Kafka topic:

- Document identity (*_id* field) will be given by a generated UUID.
- Value projection will be done using a blacklist approach in order to remove
  fields *k2* and *k4*.
- At most 100 documents will be written to the MongoDB collection
  'collectionA' in one bulk write operation.

Then there are also individual settings for topic 'topicC':

.. code-block::

   # Specific processing settings for 'topicC'

   topic.override.topicA.collection=collectionC
   topic.override.topicC.document.id.strategy=com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInValueStrategy
   topic.override.topicC.post.processor.chain=com.mongodb.kafka.connect.sink.processor.WhitelistValueProjector
   topic.override.topicC.value.projection.type=whitelist
   topic.override.topicC.value.projection.list=k3,k5
   topic.override.topicC.writemodel.strategy=com.mongodb.kafka.connect.sink.writemodel.strategy.UpdateOneTimestampsStrategy

These settings result in the following actions for messages originating from
the 'topicC' Kafka topic:

- Document identity (*_id* field) will be taken from the value structure
  of the message.
- Value projection will be done using a whitelist approach to remove only
  retain *k3* and *k5*.
- The chosen write model strategy will keep track of inserted and modified 
  timestamps for each written document.

Fallback to Defaults
~~~~~~~~~~~~~~~~~~~~

All default configurations will be used, unless a specific topic override 
is configured.

Supported Data Formats
----------------------

The sink connector implementation is configurable in order to support:

- **AVRO** (makes use of Confluent's Kafka Schema Registry and is the
  recommended format)
- **JSON with Schema** (offers JSON record structure with explicit schema 
  information)
- **JSON plain** (offers JSON record structure without any attached schema)
- **RAW JSON** (string only - JSON structure not managed by Kafka connect)

Since key and value settings can be independently configured, it is possible 
to work with different data formats for records' keys and values respectively.

*Note: Even when using RAW JSON mode i.e. with `StringConverter
<https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/connect/storage/StringConverter.html>`_
the expected Strings have to be valid JSON.*

See the excellent Confluent post `Serializers Explained
<https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained>`_ 
for more information about Kafka data serialization.

Supported Sink Record Structure
-------------------------------

Currently the connector is able to process Kafka Connect SinkRecords with 
support for the following schema types `Schema.Type
<https://kafka.apache.org/21/javadoc/org/apache/kafka/connect/data/Schema.Type.html>`_:
*INT8, INT16, INT32, INT64, FLOAT32, FLOAT64, BOOLEAN, STRING, BYTES,
ARRAY, MAP, STRUCT.*

The conversion is able to generically deal with nested key or value structures
based on the supported types listed above. The following example is based on
`AVRO <https://avro.apache.org/>`_:

.. code-block::
   
   {"type": "record",
     "name": "Customer",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
     "fields": [
       {"name": "name", "type": "string"},
       {"name": "age", "type": "int"},
       {"name": "active", "type": "boolean"},
       {"name": "address", "type":
         {"type": "record",
           "name": "AddressRecord",
           "fields": [
             {"name": "city", "type": "string"},
             {"name": "country", "type": "string"}
           ]
         }
       },
       {"name": "food", "type": 
         {"type": "array", "items": "string"}
       },
       {"name": "data", "type":
         {"type": "array", "items":
           {"type": "record",
             "name": "Whatever",
             "fields": [
               {"name": "k", "type": "string"},
               {"name": "v", "type": "int"}
             ]
           }
         }
       },
       {"name": "lut", "type": 
         {"type": "map", "values": "double"}
       },
       {"name": "raw", "type": "bytes"}
     ]
   }

Logical Types
~~~~~~~~~~~~~

Besides the standard types it is possible to use `AVRO logical types
<http://avro.apache.org/docs/1.8.2/spec.html#Logical+Types>`_ in order to
have field type support for:

- **Decimal**
- **Date**
- **Time** (millis/micros)
- **Timestamp** (millis/micros)

The following AVRO schema snippet based on exemplary logical type 
definitions should make this clearer:

.. code-block::
   {
     "type": "record",
     "name": "MyLogicalTypesRecord",
     "namespace": "com.mongodb.kafka.data.kafka.avro",
       "fields": [
         {
           "name": "myDecimalField",
           "type": {
             "type": "bytes",
             "logicalType": "decimal",
             "connect.parameters": {
               "scale": "2"
             }
           }
         },
         {
           "name": "myDateField",
           "type": {
             "type": "int",
             "logicalType": "date"
           }
         },
         {
           "name": "myTimeMillisField",
           "type": {
             "type": "int",
             "logicalType": "time-millis"
           }
         },
         {
           "name": "myTimeMicrosField",
           "type": {
             "type": "long",
             "logicalType": "time-micros"
           }
         },
         {
           "name": "myTimestampMillisField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-millis"
           }
         },
         {
           "name": "myTimestampMicrosField",
           "type": {
             "type": "long",
             "logicalType": "timestamp-micros"
           }
         }
       ]
     }

Note: If you are using AVRO code generation for logical types in order to
use them from a Java-based producer app you end-up with the following Java
type mappings:

- org.joda.time.LocalDate myDateField
- org.joda.time.LocalTime mytimeMillisField
- long myTimeMicrosField
- org.joda.time.DateTime myTimestampMillisField
- long myTimestampMicrosField

See `this discussion <https://github.com/mongodb/mongo-kafka/issues/5>`_ if
you are interested in some more details. 

Note: AVRO 1.9.0 will support native Java 8 date time types. Logical types 
can only be supported for **AVRO** and **JSON + Schema** data.

Configuration example for AVRO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block::

   key.converter=io.confluent.connect.avro.AvroConverter
   key.converter.schema.registry.url=http://localhost:8081

   value.converter=io.confluent.connect.avro.AvroConverter
   value.converter.schema.registry.url=http://localhost:8081


Configuration example for JSON with Schema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block::

   key.converter=org.apache.kafka.connect.json.JsonConverter
   key.converter.schemas.enable=true

   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=true

Post Processing of Documents
----------------------------



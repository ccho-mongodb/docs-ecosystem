.. _kafka-source:

============================
Kafka Source Connector Guide
============================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

The MongoDB Kafka Source Connector moves data from a MongoDB replica set
into a Kafka cluster. The connector configures and consumes :manual:`change
stream event documents </changeStreams>` and publishes them to a topic.

Change streams, a feature introduced in MongoDB 3.6, generate real-time events
that contain changes to data stored in MongoDB with guarantees of
durability, security, and idempotency. You can configure change streams to
observe changes at the **collection**, **database**, or **client** level. See
`An Introduction to Change Streams
<https://www.mongodb.com/blog/post/an-introduction-to-change-streams>`_
for more information.

.. note::

   Change streams require a replicaSet or a sharded cluster using replicaSets.

Change Stream Event Document Format
-----------------------------------

A change stream event document contains several fields that describe the
change.

* The top-level ``_id`` field is used as the :manual:`resume token
  </changeStreams/#change-stream-resume>` which is
  used to start a change stream from a specific point in time.

* The ``operationType`` field identifies the type of change represented in
  the change stream document. Possible values include: "insert", "update",
  "replace", "delete", "invalidate", "drop", "dropDatabase", and "rename".

* The ``fullDocument`` field contains the following, depending on the
  operation:

  - For insert and replace operations, it contains the new document being
    inserted or replacing the existing document.
  - For update operations, it contains the complete document that is being
    updated at some point in time after the update occurred. If the
    document was deleted since the update, it contains a null value.

* The ``documentKey`` contains either the ``_id`` field of the document
  that was updated or all the components of a shard key for sharded
  collections.

* The ``txnNumber`` and ``lsid`` provide data describing the transaction
  if the change occurred within one.

.. code-block:: json

   {
     _id: { <BSON Object> },
     "operationType": "<operation>",
     "fullDocument": { <document> },
     "ns": {
       "db": <database>,
       "coll": <collection>
     },
     "to": {
       "db": <database>,
       "coll": <collection>
     },
     "documentKey": {
       _id: <value>
     },
     "updateDescription": {
       "updatedFields": { <document> },
       "removedFields": [ <field>, ... ]
     },
     "clusterTime": <Timestamp>,
     "txnNumber": <NumberLong>,
     "lsid": {
       "id": <UUID>,
       "uid": <BinData>
     }
   }

Source Connector Configuration Properties
-----------------------------------------

This section lists the available configuration settings used to compose a
properties file for the MongoDB Kafka Source Connector. The connector uses
these settings to create change streams and customize the output to save
to the Kafka cluster. For an example source connector configuration file, see
`MongoSourceConnector.properties
<https://github.com/mongodb/mongo-kafka/blob/master/config/MongoSourceConnector.properties>`_.

.. list-table::
   :header-rows: 1
   :stub-columns: 1

   * - Name
     - Type
     - Description

   * - connection.uri
     - string
     - | A :manual:`MongoDB connection URI string </reference/connection-string/#standard-connection-string-format>`.
       | **Default**: ``mongodb://localhost:27017,localhost:27018,localhost:27019``
       | **Accepted Values**: A valid MongoDB connection URI string

   * - database
     - string
     - | Name of the database to watch. If not set, all databases are watched.
       | **Default**: ""
       | **Accepted Values**: A single database name

   * - collection
     - string
     - | Name of the collection in the database to watch.
       | The collection in the database to watch. If not set then all collections will be watched.
       | **Default**: ""
       | **Accepted Values**: A single collection name

   * - publish.full.document.only
     - boolean
     - | Only publish the changed document instead of the full change stream document. Sets the ``change.stream.full.document=updateLookup`` automatically so updated documents will be included.

       .. seealso:: :ref:`Publish full document example <full-document-example>`

       | **Default**: false
       | **Accepted Values**: ``true`` or ``false``

   * - pipeline
     - string
     - | An array of objects describing the pipeline operations to run.

       .. example::

          .. code-block:: shell

             [{"$match": {"operationType": "insert"}}, {"$addFields": {"Kafka": "Rules!"}}]

       .. seealso:: :ref:`Custom pipeline example <custom-pipeline-example>`.

       | **Default**: []
       | **Accepted Values**: A valid JSON array

   * - collation
     - string
     - | A JSON :manual:`collation document </reference/collation/#collation-document>` that contains options to use for the change stream. Append ``.asDocument().toJson()`` to the collation document to create the JSON representation.
       | **Default**: ""
       | **Accepted Values**: A valid JSON document representing a collection

   * - batch.size
     - int
     - | The cursor batch size.
       | **Default**: 0
       | **Accepted Values**: An integer

   * - change.stream.full.document
     - string
     - | Determines what to return for update operations when using a Change Stream. When set to 'updateLookup', the change stream for partial updates will include both a delta describing the changes to the document as well as a copy of the entire document that was changed from *some point in time* after the change occurred.
       | **Default**: ""
       | **Accepted Values**: "" or ``default`` or ``updatelookup``

   * - poll.await.time.ms
     - long
     - | The amount of time to wait before checking for new results on the change stream
       | **Default**: 5000
       | **Accepted Values**: An integer

   * - poll.max.batch.size
     - int
     - | Maximum number of change stream documents to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.
       | **Default**: 1000
       | **Accepted Values**: An integer

   * - topic.prefix
     - string
     - | Prefix to prepend to database & collection names to generate the name of the Kafka topic to publish data to.

       .. seealso:: :ref:`Topic naming example <topic-naming-example>`.

       | **Default**: ""
       | **Accepted Values**: A string

.. note::

   The default maximum size for Kafka messages is 1MB. Update the
   following Kafka (versions 0.11.0 through 2.2) configuration properties to
   enable a larger maximum size if the JSON string size of the change stream
   documents exceeds the maximum:

   .. list-table::
      :header-rows: 1
      :stub-columns: 2

      * - System
        - Property Name
        - Description

      * - Consumer
        - `max.partition.fetch.bytes
          <https://kafka.apache.org/22/documentation.html#consumerconfigs>`_
        - Maximum size of a message that can be fetched by a consumer.

      * - Broker
        - `replica.fetch.max.bytes
          <https://kafka.apache.org/22/documentation.html#brokerconfigs>`_
        - Maximum size of a message that can be replicated within a Kafka
          cluster.

      * - Broker
        - `message.max.bytes
          <https://kafka.apache.org/22/documentation.html#brokerconfigs>`_
        - Maximum size of a message from a producer that is accepted by the
          broker.

      * - Producer
        - `max.message.bytes
          <https://kafka.apache.org/22/documentation.html#producerconfigs>`_
        - Per referenced topic, the maximum size of an uncompressed message that can be appended to a
          topic.

.. _full-document-example:

Publish Changed Documents Only
------------------------------

We can set the source connector to publish only the ``fullDocument`` field
rather than the entire change stream event document with the following
setting:

.. code-block:: properties

   publish.full.document.only=true

However, certain types of updates may not include a ``fullDocument``
field. Therefore, the ``change.stream.full.document=updateLookup`` setting
which will only publish events that contain a ``fullDocument`` field which
contains a resume token. ??? TODO

actual documents after inserts, replaces or updates.

.. code-block:: properties

   publish.full.document.only=true

This will automatically configure ``change.stream.full.document=updateLookup``
and will only publish events that contain a ``fullDocument`` field.

.. _custom-pipeline-example:

Custom Pipeline Example
-----------------------

We can use the ``pipeline`` configuration setting to define an
:manual:`aggregation pipeline <reference/operator/aggregation-pipeline/>`
to filter or modify the change events output. In this example, we set the
``pipeline`` configuration to observe only insert change events:

.. code-block:: properties

   pipeline=[{"$match": {"operationType": "insert"}}]

.. note::

   MongoDB requires all change stream documents to include the top-level
   ``_id`` field which it uses as a :manual:`resume token
   </changeStreams/#change-stream-resume-token/>`.


.. _topic-naming-example:

Topic Naming Example
--------------------

The MongoDB Kafka Source connector publishes the changed data events to
a Kafka topic that consists of the database and collection name from which
the change originated. For example, if an insert was performed on the
``test`` database and ``data`` collection, the connector will publish the
data to a topic named: ``test.data``.

If the ``topic.prefix`` configuration is set, the Kafka topic name will be
prepended with the specified value. For example:

.. code-block:: properties

   topic.prefix=mongo

With the configuration setting above, any data changes to the ``data``
collection in the ``test`` database are published to a topic named
``mongo.test.data``.

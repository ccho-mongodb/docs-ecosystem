.. _kafka-source:

============================
Kafka Source Connector Guide
============================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

The MongoDB Kafka Source Connector imports data from a MongoDB replica set
into a Kafka cluster from which it can be published onto a topic. The
MongoDB data is generated by :manual:`change stream event documents
</changeStreams>`.

Change streams can observe changes at the *collection*\ , *database* or *client* level.

Data is read from MongoDB using the configuration connection as specified in the
`connection string <http://mongodb.github.io/mongo-java-driver/3.10/javadoc/com/mongodb/ConnectionString.html>`_.

**Note:** Change streams require a replicaSet or a sharded cluster using replicaSets.

Currently, only JSON strings are supported as the output.

.. note::

   The default maximum size for Kafka messages is 1MB. Update the
   following Kafka (versions 0.11.0 through 2.2) configuration properties to
   enable a larger maximum size if the JSON string size of the change stream
   documents exceeds the maximum:

   .. list-table::
      :header-rows: 1
      :stub-columns: 2

      * - System
        - Property Name
        - Description

      * - Consumer
        - `max.partition.fetch.bytes
          <https://kafka.apache.org/22/documentation.html#consumerconfigs>`_
        - Maximum size of a message that can be fetched by a consumer.

      * - Broker
        - `replica.fetch.max.bytes
          <https://kafka.apache.org/22/documentation.html#brokerconfigs>`_
        - Maximum size of a message that can be replicated within a Kafka
          cluster.

      * - Broker
        - `message.max.bytes
          <https://kafka.apache.org/22/documentation.html#brokerconfigs>`_
        - Maximum size of a message from a producer that is accepted by the
          broker.

      * - Producer
        - `max.message.bytes
          <https://kafka.apache.org/22/documentation.html#producerconfigs>`_
        - Per referenced topic, the maximum size of an uncompressed message that can be appended to a
          topic.

Event Document Format
---------------------

The following document represents all possible fields that a change stream
response document can have:

.. code-block:: json

   {
     _id: { <BSON Object> },
     "operationType": "<operation>",
     "fullDocument": { <document> },
     "ns": {
       "db": <database>,
       "coll": <collection>
     },
     "to": {
       "db": <database>,
       "coll": <collection>
     },
     "documentKey": {
       _id: <value>
     },
     "updateDescription": {
       "updatedFields": { <document> },
       "removedFields": [ <field>, ... ]
     },
     "clusterTime": <Timestamp>,
     "txnNumber": <NumberLong>,
     "lsid": {
       "id": <UUID>,
       "uid": <BinData>
     }
   }

Source Connector Configuration Properties
-----------------------------------------

.. list-table::
   :header-rows: 1
   :stub-columns: 1

   * - Name
     - Type
     - Description

   * - connection.uri
     - string
     - | A :manual:`MongoDB connection URI string </reference/connection-string/#standard-connection-string-format>`.
       | **Default**: ``mongodb://localhost:27017,localhost:27018,localhost:27019``
       | **Accepted Values**: A valid MongoDB connection URI string

   * - database
     - string
     - | Name of the database to watch. If not set, all databases are watched.
       | **Default**: ""
       | **Accepted Values**: A single database name

   * - collection
     - string
     - | Name of the collection in the database to watch.
       | The collection in the database to watch. If not set then all collections will be watched.
       | **Default**: ""
       | **Accepted Values**: A single collection name

   * - publish.full.document.only
     - boolean
     - | Only publish the changed document instead of the full change stream document. Sets the ``change.stream.full.document=updateLookup`` automatically so updated documents will be included.

       .. seealso:: :ref:`Publish full document example <full-document-example>`

       | **Default**: false
       | **Accepted Values**: ``true`` or ``false``

   * - pipeline
     - string
     - | An array of objects describing the pipeline operations to run.

       .. example::

          .. code-block:: shell

             [{"$match": {"operationType": "insert"}}, {"$addFields": {"Kafka": "Rules!"}}]

       .. seealso:: :ref:`Custom pipeline example <custom-pipeline-example>`.

       | **Default**: []
       | **Accepted Values**: A valid JSON array

   * - collation
     - string
     - | A JSON :manual:`collation document </reference/collation/#collation-document>` that contains options to use for the change stream. Append ``.asDocument().toJson()`` to the collation document to create the JSON representation.
       | **Default**: ""
       | **Accepted Values**: A valid JSON document representing a collection

   * - batch.size
     - int
     - | The cursor batch size.
       | **Default**: 0
       | **Accepted Values**: An integer

   * - change.stream.full.document
     - string
     - | Determines what to return for update operations when using a Change Stream. When set to 'updateLookup', the change stream for partial updates will include both a delta describing the changes to the document as well as a copy of the entire document that was changed from *some time* after the change occurred.
       | **Default**: ""
       | **Accepted Values**: "" or ``default`` or ``updatelookup``

   * - poll.await.time.ms
     - long
     - | The amount of time to wait before checking for new results on the change stream
       | **Default**: 5000
       | **Accepted Values**: An integer

   * - poll.max.batch.size
     - int
     - | Maximum number of change stream documents to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.
       | **Default**: 1000
       | **Accepted Values**: An integer

   * - topic.prefix
     - string
     - | Prefix to prepend to database & collection names to generate the name of the Kafka topic to publish data to.

       .. seealso:: :ref:`Topic naming example <topic-naming-example>`.

       | **Default**: ""
       | **Accepted Values**: A string

.. _custom-pipeline-example:

Custom pipeline Example
-----------------------

TODO: rewrite
The following example can be used to only observe inserted files:

.. code-block:: properties

   pipeline=[{"$match": {"operationType": "insert"}}]

*Note:* MongoDB requires all change stream documents to include the resume token - the top level ``_id`` field.

.. _full-document-example:

Publishing Changed Documents Only
---------------------------------

Due to the restriction of requiring the resume token, a special configuration option has been added that allows users to only publish the
actual documents after inserts, replaces or updates.

.. code-block:: properties

   publish.full.document.only=true

This will automatically configure ``change.stream.full.document=updateLookup`` and will only publish events that contain a ``fullDocument`` field.


.. _topic-naming-example:

Topic Naming Example
--------------------

The MongoDB Kafka Source connector publishes the changed data events to
a Kafka topic that consists of the database and collection name from which
the change originated. For example, if an insert was performed on the
``test`` database and ``data`` collection, the connector will publish the
data to a topic named: ``test.data``.

If the ``topic.prefix`` configuration is set, the Kafka topic name will be
prefixed with the specified value. For example:

.. code-block:: properties

   topic.prefix=mongo

With the configuration setting above, any data changes to the ``data``
collection in the ``test`` database  are published to a topic named
``mongo.test.data``.
